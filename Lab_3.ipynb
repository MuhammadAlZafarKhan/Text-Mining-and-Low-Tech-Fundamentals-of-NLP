{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# ***Lab 3: Text Preprocessing***"
      ],
      "metadata": {
        "id": "1zK02eu4O1PR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 1:** Lowercasing"
      ],
      "metadata": {
        "id": "SO7VyLSzSO54"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zH5sFJLwOwAk",
        "outputId": "638cac86-898e-45ff-dd2b-e9269cac016b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: The Product Quality is Excellent!\n",
            "Lowercased: the product quality is excellent!\n",
            "--------------------------------------------------\n",
            "Original: CUSTOMER Service was Terrible.\n",
            "Lowercased: customer service was terrible.\n",
            "--------------------------------------------------\n",
            "Original: Fast Delivery and Great Packaging.\n",
            "Lowercased: fast delivery and great packaging.\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Example customer reviews\n",
        "reviews = [\n",
        "    \"The Product Quality is Excellent!\",\n",
        "    \"CUSTOMER Service was Terrible.\",\n",
        "    \"Fast Delivery and Great Packaging.\"\n",
        "]\n",
        "\n",
        "# Apply lowercasing\n",
        "lowercased_reviews = [review.lower() for review in reviews]\n",
        "\n",
        "# Display results\n",
        "for original, lowered in zip(reviews, lowercased_reviews):\n",
        "    print(f\"Original: {original}\")\n",
        "    print(f\"Lowercased: {lowered}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 2:** Removing Punctuation and Special Characters"
      ],
      "metadata": {
        "id": "zeEIWWaMSbu9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Example customer reviews\n",
        "reviews = [\n",
        "    \"The product quality is excellent!!! 😍\",\n",
        "    \"Customer service was terrible, not happy... :(\",\n",
        "    \"Fast delivery & great packaging @GlobalMart\",\n",
        "    \"Refund requested due to damaged item #12345.\"\n",
        "]\n",
        "\n",
        "# Function to remove punctuation and special characters\n",
        "def remove_punctuations(text):\n",
        "    # Keep only letters, numbers, and spaces\n",
        "    return re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "\n",
        "# Apply cleaning\n",
        "cleaned_reviews = [remove_punctuations(review) for review in reviews]\n",
        "\n",
        "# Display results\n",
        "for original, cleaned in zip(reviews, cleaned_reviews):\n",
        "    print(f\"Original:   {original}\")\n",
        "    print(f\"Cleaned:    {cleaned}\")\n",
        "    print(\"-\" * 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTdr5_ZdSVxt",
        "outputId": "323fa9d5-ae8c-4215-d2a0-bfd419682bdf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original:   The product quality is excellent!!! 😍\n",
            "Cleaned:    The product quality is excellent \n",
            "--------------------------------------------------\n",
            "Original:   Customer service was terrible, not happy... :(\n",
            "Cleaned:    Customer service was terrible not happy \n",
            "--------------------------------------------------\n",
            "Original:   Fast delivery & great packaging @GlobalMart\n",
            "Cleaned:    Fast delivery  great packaging GlobalMart\n",
            "--------------------------------------------------\n",
            "Original:   Refund requested due to damaged item #12345.\n",
            "Cleaned:    Refund requested due to damaged item 12345\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 3:** Removing Stopwords"
      ],
      "metadata": {
        "id": "HZGsQPLrS9mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "# Download stopwords (only needed once)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Sample text\n",
        "text = \"This is an example sentence, showing how to remove stopwords and punctuation!\"\n",
        "\n",
        "# Convert text to lowercase\n",
        "text = text.lower()\n",
        "\n",
        "# Remove punctuation\n",
        "text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "# Tokenize (split into words)\n",
        "words = text.split()\n",
        "\n",
        "# Get English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Separate stopwords and non-stopwords\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "removed_stopwords = [word for word in words if word in stop_words]\n",
        "\n",
        "print(\"Original words:\", words)\n",
        "print(\"Filtered words (stopwords removed):\", filtered_words)\n",
        "print(\"Stopwords removed:\", removed_stopwords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNPb-F85TFaZ",
        "outputId": "6f0a00fe-79e4-4e40-8073-ead0bef236e5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['this', 'is', 'an', 'example', 'sentence', 'showing', 'how', 'to', 'remove', 'stopwords', 'and', 'punctuation']\n",
            "Filtered words (stopwords removed): ['example', 'sentence', 'showing', 'remove', 'stopwords', 'punctuation']\n",
            "Stopwords removed: ['this', 'is', 'an', 'how', 'to', 'and']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 4:** Stemming"
      ],
      "metadata": {
        "id": "4U-pgkk3TxVD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Initialize stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Sample text\n",
        "text = \"The customers are running, studies show they were happier after receiving their products.\"\n",
        "\n",
        "# Simple tokenization using regex (no nltk punkt needed)\n",
        "words = re.findall(r\"\\b\\w+\\b\", text.lower())  # extract words only\n",
        "\n",
        "# Apply stemming\n",
        "stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "# Compare original vs stemmed\n",
        "print(\"Original words:\", words)\n",
        "print(\"Stemmed words:\", stemmed_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9SIzbaHTt-j",
        "outputId": "f20bb689-f433-40c9-f449-4c1d94ffc021"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original words: ['the', 'customers', 'are', 'running', 'studies', 'show', 'they', 'were', 'happier', 'after', 'receiving', 'their', 'products']\n",
            "Stemmed words: ['the', 'custom', 'are', 'run', 'studi', 'show', 'they', 'were', 'happier', 'after', 'receiv', 'their', 'product']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 5:** Lemmatization"
      ],
      "metadata": {
        "id": "lFZsrqOQXDUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install spaCy model (only first time, then comment this out)\n",
        "# !python -m spacy download en_core_web_sm\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Sample text\n",
        "text = \"The customers are running, studies show they were happier after receiving their products.\"\n",
        "\n",
        "# Process text with spaCy\n",
        "doc = nlp(text)\n",
        "\n",
        "# Extract original tokens and their lemmas\n",
        "lemmas = [(token.text, token.lemma_) for token in doc if token.is_alpha]\n",
        "\n",
        "# Print results\n",
        "print(\"Original vs Lemmatized:\")\n",
        "for word, lemma in lemmas:\n",
        "    print(f\"{word:15} --> {lemma}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tm4FpRgLW_Mt",
        "outputId": "46a81eaa-0805-4102-9c5d-8e9e8664b88d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original vs Lemmatized:\n",
            "The             --> the\n",
            "customers       --> customer\n",
            "are             --> be\n",
            "running         --> run\n",
            "studies         --> study\n",
            "show            --> show\n",
            "they            --> they\n",
            "were            --> be\n",
            "happier         --> happy\n",
            "after           --> after\n",
            "receiving       --> receive\n",
            "their           --> their\n",
            "products        --> product\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 6:** Text Normalization"
      ],
      "metadata": {
        "id": "ZSikQ08JYkAQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import spacy\n",
        "import string\n",
        "\n",
        "# Load spaCy English model for lemmatization\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Business text example\n",
        "text = \"\"\"\n",
        "Hi John, thanks for ordering from BusinessCorp!\n",
        "Your order #12345 was placed on 12/09/2025 for $499.99.\n",
        "Visit https://businesscorp.com/orders/12345 to track it.\n",
        "If you have questions, email support@businesscorp.com or call +971-50-123-4567.\n",
        "We’re offering 10% off your next order—don’t miss it!\n",
        "\"\"\"\n",
        "\n",
        "# Dictionary for contractions\n",
        "contractions = {\n",
        "    \"don’t\": \"do not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"won’t\": \"will not\",\n",
        "    \"i'm\": \"i am\",\n",
        "    \"we’re\": \"we are\",\n",
        "    \"they’re\": \"they are\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"you're\": \"you are\",\n",
        "}\n",
        "\n",
        "def expand_contractions(text, contractions_dict):\n",
        "    pattern = re.compile(\"|\".join(re.escape(k) for k in contractions_dict.keys()), flags=re.IGNORECASE)\n",
        "    return pattern.sub(lambda m: contractions_dict[m.group(0).lower()], text)\n",
        "\n",
        "def normalize_text(text):\n",
        "    # 1. Lowercasing\n",
        "    text = text.lower()\n",
        "\n",
        "    # 2. Expand contractions\n",
        "    text = expand_contractions(text, contractions)\n",
        "\n",
        "    # 3. Replace business entities\n",
        "    text = re.sub(r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\", \"<EMAIL>\", text)   # Emails\n",
        "    text = re.sub(r\"http[s]?://\\S+\", \"<URL>\", text)                                          # URLs\n",
        "    text = re.sub(r\"\\+?\\d{1,3}[-\\s]?\\(?\\d{1,4}\\)?[-\\s]?\\d{3}[-\\s]?\\d{4}\", \"<PHONE>\", text)   # Phone numbers\n",
        "    text = re.sub(r\"\\b\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4}\\b\", \"<DATE>\", text)                      # Dates\n",
        "    text = re.sub(r\"\\$\\d+(?:\\.\\d{2})?\", \"<MONEY>\", text)                                     # Money\n",
        "    text = re.sub(r\"\\d+(\\.\\d+)?%\", \"<PERCENT>\", text)                                        # Percentages\n",
        "    text = re.sub(r\"\\b\\d+\\b\", \"<NUM>\", text)                                                 # Plain numbers\n",
        "\n",
        "    # 4. Remove punctuation\n",
        "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
        "\n",
        "    # 5. Lemmatization\n",
        "    doc = nlp(text)\n",
        "    text = \" \".join([token.lemma_ for token in doc if token.is_alpha])\n",
        "\n",
        "    return text\n",
        "\n",
        "# Apply normalization\n",
        "normalized = normalize_text(text)\n",
        "\n",
        "print(\"Original Text:\\n\", text)\n",
        "print(\"\\nNormalized Text:\\n\", normalized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AKmqilzeYoVG",
        "outputId": "9c94ca08-1770-44a2-9ad1-5d678ee75466"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            " \n",
            "Hi John, thanks for ordering from BusinessCorp! \n",
            "Your order #12345 was placed on 12/09/2025 for $499.99.\n",
            "Visit https://businesscorp.com/orders/12345 to track it.\n",
            "If you have questions, email support@businesscorp.com or call +971-50-123-4567.\n",
            "We’re offering 10% off your next order—don’t miss it!\n",
            "\n",
            "\n",
            "Normalized Text:\n",
            " hi john thank for order from businesscorp your order NUM be place on date for money visit url to track it if you have question email email or call phone we be offer PERCENT off your next order do not miss it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Exercise 7:** Handlings Numbers, Dates, URLs, Emails, etc."
      ],
      "metadata": {
        "id": "K79Z2FNSXyfN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Sample business text\n",
        "text = \"\"\"\n",
        "Customer John.Doe@example.com placed an order on 12/09/2025 for $499.99.\n",
        "Visit https://example.com/order/12345 to track it.\n",
        "Contact us at support@business.com or +971-50-123-4567.\n",
        "The delivery success rate was 98.5% in Q3.\n",
        "\"\"\"\n",
        "\n",
        "# Define regex patterns\n",
        "patterns = {\n",
        "    \"EMAIL\": r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b\",\n",
        "    \"URL\": r\"http[s]?://\\S+\",\n",
        "    \"DATE\": r\"\\b(?:\\d{1,2}[/-]\\d{1,2}[/-]\\d{2,4})\\b\",\n",
        "    \"PHONE\": r\"\\+?\\d{1,3}[-\\s]?\\(?\\d{1,4}\\)?[-\\s]?\\d{3}[-\\s]?\\d{4}\",\n",
        "    \"MONEY\": r\"\\$\\d+(?:\\.\\d{2})?\",\n",
        "    \"PERCENT\": r\"\\d+(\\.\\d+)?%\",\n",
        "    \"NUMBER\": r\"\\b\\d+\\b\"\n",
        "}\n",
        "\n",
        "# Function to normalize text\n",
        "def normalize_text(text):\n",
        "    for label, pattern in patterns.items():\n",
        "        text = re.sub(pattern, f\"<{label}>\", text)\n",
        "    return text\n",
        "\n",
        "# Apply normalization\n",
        "normalized_text = normalize_text(text)\n",
        "\n",
        "print(\"Original Text:\\n\", text)\n",
        "print(\"\\nNormalized Text:\\n\", normalized_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6bYqG2CqXtOd",
        "outputId": "b63a7f50-58da-489c-8c73-1515f00f514b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            " \n",
            "Customer John.Doe@example.com placed an order on 12/09/2025 for $499.99.\n",
            "Visit https://example.com/order/12345 to track it. \n",
            "Contact us at support@business.com or +971-50-123-4567.\n",
            "The delivery success rate was 98.5% in Q3.\n",
            "\n",
            "\n",
            "Normalized Text:\n",
            " \n",
            "Customer <EMAIL> placed an order on <DATE> for <MONEY>.\n",
            "Visit <URL> to track it. \n",
            "Contact us at <EMAIL> or <PHONE>.\n",
            "The delivery success rate was <PERCENT> in Q3.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}